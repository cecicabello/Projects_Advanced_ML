{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Projects in Advanced Machine Learning**\n",
    "### **GR5074**\n",
    "### **Projects' portafolio**\n",
    "### **Cecilia Cabello EsquerÂ¶**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Assignment 1 (Happiness score)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the first glance at the course and it focused on developing machine learning models to predict Happiness scores for world regions given their quality of living. The target variable, Happiness Score, was an ordinal variable with five categories, ranging from Very High to Very low. The rest of the variables were numerical and they included GDP per capita, Social support, Healthy life expectancy, freedom to make life choices, Generosity, and Perceptions of corruption. The project also took focus on the use of feature selection methods, to determine what the best parameters were for each model. \n",
    "    \n",
    "When exploring the relationships between the target variable (Happiness level) and the particular features used by the United Nations calculations, it was shown that a positive correlation (and therefore likely influence) existed with GDP per capita, Health expectancy, and Social Support. All of which are commonly considered to have colinearity and depend on the income level of the population. Hence, these results are not surprising. However, the relationship between Happiness and Generosity is not as clear, given the variations and averages on each level of the target variable.\n",
    "     \n",
    "After having run two models focused on feature selection: Random Forest Regressor and Lasso, combined with Select from Model, there is no consensus between the two as the Random Forest Regressor does not zeroed any variables. In contrast, Lasso, zeroed Generosity and Perception of corruption, leaving only 9 features as relevant in the prediction. \n",
    "\n",
    "As predicting models three approaches were attempted a Neural Network with two hidden layers of 12 neurons, a Random Forest Classifier with a regularization parameter of c=1000, and a Support Vector Classifier (SVC). Of which, the one with the best performance was tied between the neural network and the support vector classifier, both getting an accuracy score of 0.461538. The SVC got a better precision score (0.538 - 0.483), while the opposite happens with recall (SVC-0.475, NN-0.4777).  \n",
    " \n",
    "Finally, the Random Forest model was even lower than the previous two, given the low scores across the board we concluded that the information available was not sufficient to result in a reliable prediction. This could address in two ways, one is by including additional features to the dataset, such as pollution, homicides, and other proxies for quality of living. The second idea is a little more complex as it would require the model to deal with time series. As I would include the same model but with more years since this one only uses 2019 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://github.com/cecicabello/Projects_Advanced_ML/tree/master/HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Assignment 2 (Brain Cancer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second assignment focused on neural networks for image detection, the main goal of the assignment was to create models to predict the existence of brain tumors using MRI images. So a yes or no answer. The example is interesting as having the ability to detect the existence of brain tumors, using only magnetic resonance imaging (MRIs) could minimize the diagnosis time, thereby improving the efficiency of set diagnosis. Although such prediction models might not be enough to determine the existence of a life-altering disease (given their accuracy scores), these do contribute greatly to the optimization of the whole diagnosis process, and could even be included as another verification layer or step. Such a model would benefit mostly those involved in the diagnosis process, particularly doctors, as it would shorten the response time for patients, resulting in higher levels of certainty, and might even automatize a part of an everyday task for many health care workers.\n",
    "\n",
    "Three neural networks with different specifications such as layers, dropout, and regularization methods where used, at the same time one of the most important parts of this particular assignment was the preprocessing of the input, images.\n",
    "\n",
    "The model with the highest accuracy scores was a simple VGG19, three layers with no dropout nor another regularization method, and stochastic gradient descent as an optimizer with a learning rate of 0.0001. Using 25 epochs, it resulted in an accuracy score of 82%. Improving from the previous model, which held the same conditions, except for an additional layer and a lower number of epochs, 5. Said model held an accuracy score of 74%.\n",
    "\n",
    "The final model for this assignment was a more simple approach as it was a neural network with no regularization method, two layers, 25 epochs, and no checkout. It obtained a 76% accuracy score. The simpler model had better results than the more complicated one, hereby we could assume the simpler the model the better the scores, for the available data.\n",
    "\n",
    "Even though the accuracy score gets better from one model to the next 84% accuracy seems still really low as we are talking about people's health and the recommendations given by physicians. For an issue of such relevance in one's life, I would consider even other more accurate models still requiring human supervision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link:** https://github.com/cecicabello/Projects_Advanced_ML/tree/master/HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Assignment 3 (News Category)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final assignment of the semester focused on the development of recurring neural networks to categorize news based on text, using an NBC News Desk dataset. The news could be assigned to five different categories: sports, business, politics, tech, and entertainment, based only on the text describing the situation. It is clear given the exploratory analysis, that business and sport are the two most common categories and that the extent of the text and words could vary a lot.\n",
    "    \n",
    "As in the previous two cases, before engaging in modeling, data was preprocessed and encoded to fit in the model. Since the input for the prediction was text, a couple of new considerations were taken, such as establishing a limit to the length allowed from each text. But most importantly the incorporation of an embedding layer, which allows for the addition of continuous features to text data. Trying to establish differences in the distance as a function of the similarities or differences between words. Therefore, all the models for this assignment required the use of an embedding layer as an initial layer to the neural networks. \n",
    "\n",
    "Out of the three more complex models explore in this analysis the one with the best results (0.8204) was the bidirectional model using an LSTM sequential layer and one dense layer with 200 neurons. Such a result is as expected since in bidirectional models texts are read one time and then also read in the opposite direction, as the word after it might change the meaning of the word in question. \n",
    "\n",
    "In an attempt to better the accuracy of the model we included a dropout of 0.15, while compensating its possible overfitting by adding more epochs to our model (from 20 to 30). And the result was positive, increasing its accuracy to 0.86. \n",
    "\n",
    "The next model that was looked into included a single sequential layer (LSTM) and a dense layer of 200 neurons. In contrast, the second most complicated model, stacked sequential layers (using GRUs) performed much more poorly. This leads me to believe LSTM is better than GRU in this particular case. Since both LSTM and GRU try to improve recurring neural networks, given that in some cases the optimization of RNNs can break. Therefore, GRU and LSTM are used to carry forward information from earlier in the network. \n",
    "\n",
    "In contrast, to the previous two assignments, I believe this model would benefit greatly from the use of pre-trained neural networks as there is a wide variety of such networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link:** https://github.com/cecicabello/Projects_Advanced_ML/tree/master/HW3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
