{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQcs5w3U0NqR"
   },
   "source": [
    "### **Projects in Advanced Machine Learning**\n",
    "#### **GR5074**\n",
    "#### **Homework 1**\n",
    "#### **Cecilia Cabello Esquer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoL9hjjEYLq2"
   },
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYjULzAaVRjQ"
   },
   "source": [
    "Dataset:\n",
    "\n",
    "Features\n",
    "* Country or region\n",
    "* GDP per capita\n",
    "* Social support\n",
    "* Healthy life expectancy\n",
    "* Freedom to make life choices\n",
    "* Generosity\n",
    "* Perceptions of corruption\n",
    "\n",
    "Target\n",
    "* Happiness_level (Very High = Top 20% and Very Low = Bottom 20%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HBVRc6cYeqk"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "f28Okm3AX5GI",
    "outputId": "90bc11e5-eb62-477f-a476-76aac2b18919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qyz4OTOQYTD9"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "colab_type": "code",
    "id": "vvlZN1RnYUmY",
    "outputId": "f5095d29-278e-4ac8-c0f8-0ff17a938657"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import keras\n",
    "\n",
    "#import aimodelshare as ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIu5pwzBaYfo"
   },
   "outputs": [],
   "source": [
    "# Test-train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JM5j5QK7X5ZP"
   },
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "7XxGlatCXytf",
    "outputId": "65d6d81b-18f3-4741-d070-da570068a420"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness_level</th>\n",
       "      <th>Country or region</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Finland</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Norway</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Happiness_level Country or region  ...  Generosity  Perceptions of corruption\n",
       "0       Very High           Finland  ...       0.153                      0.393\n",
       "1       Very High           Denmark  ...       0.252                      0.410\n",
       "2       Very High            Norway  ...       0.271                      0.341\n",
       "3       Very High           Iceland  ...       0.354                      0.118\n",
       "4       Very High       Netherlands  ...       0.322                      0.298\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"worldhappiness2019.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Lv8eRi0eJPv"
   },
   "source": [
    "##### Add region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "GLGxhlHqeSBI",
    "outputId": "40482590-d0d2-4a79-912b-644b7d50365c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>alpha-2</th>\n",
       "      <th>alpha-3</th>\n",
       "      <th>country-code</th>\n",
       "      <th>iso_3166-2</th>\n",
       "      <th>region</th>\n",
       "      <th>sub-region</th>\n",
       "      <th>intermediate-region</th>\n",
       "      <th>region-code</th>\n",
       "      <th>sub-region-code</th>\n",
       "      <th>intermediate-region-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>4</td>\n",
       "      <td>ISO 3166-2:AF</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>AX</td>\n",
       "      <td>ALA</td>\n",
       "      <td>248</td>\n",
       "      <td>ISO 3166-2:AX</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>8</td>\n",
       "      <td>ISO 3166-2:AL</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>12</td>\n",
       "      <td>ISO 3166-2:DZ</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "      <td>ASM</td>\n",
       "      <td>16</td>\n",
       "      <td>ISO 3166-2:AS</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name alpha-2  ... sub-region-code  intermediate-region-code\n",
       "0     Afghanistan      AF  ...            34.0                       NaN\n",
       "1   Åland Islands      AX  ...           154.0                       NaN\n",
       "2         Albania      AL  ...            39.0                       NaN\n",
       "3         Algeria      DZ  ...            15.0                       NaN\n",
       "4  American Samoa      AS  ...            61.0                       NaN\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions= pd.read_csv('all.csv')\n",
    "regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "2VVCxv96fkpQ",
    "outputId": "5bd57d7c-1d23-4cad-eb98-35f2070a702e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 8 columns):\n",
      "Happiness_level                 156 non-null object\n",
      "Country or region               156 non-null object\n",
      "GDP per capita                  156 non-null float64\n",
      "Social support                  156 non-null float64\n",
      "Healthy life expectancy         156 non-null float64\n",
      "Freedom to make life choices    156 non-null float64\n",
      "Generosity                      156 non-null float64\n",
      "Perceptions of corruption       156 non-null float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249 entries, 0 to 248\n",
      "Data columns (total 11 columns):\n",
      "name                        249 non-null object\n",
      "alpha-2                     248 non-null object\n",
      "alpha-3                     249 non-null object\n",
      "country-code                249 non-null int64\n",
      "iso_3166-2                  249 non-null object\n",
      "region                      248 non-null object\n",
      "sub-region                  248 non-null object\n",
      "intermediate-region         107 non-null object\n",
      "region-code                 248 non-null float64\n",
      "sub-region-code             248 non-null float64\n",
      "intermediate-region-code    107 non-null float64\n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 21.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print(regions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "CpwnfYwRhXfW",
    "outputId": "5e95fc68-f0a5-4f53-ad5e-2ae0dfbb4cf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness_level</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "      <th>Country or region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very High</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very High</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very High</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very High</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very High</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Happiness_level  GDP per capita  ...  Perceptions of corruption  Country or region\n",
       "0       Very High           1.340  ...                      0.393             Europe\n",
       "1       Very High           1.383  ...                      0.410             Europe\n",
       "2       Very High           1.488  ...                      0.341             Europe\n",
       "3       Very High           1.380  ...                      0.118             Europe\n",
       "4       Very High           1.396  ...                      0.298             Europe\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge\n",
    "data= pd.merge(data, regions[[\"name\", \"region\"]], left_on='Country or region', right_on='name', how='left')\n",
    "data= data.drop(['Country or region', 'name'], axis = 1) \n",
    "data= data.rename({'region':'Country or region'}, axis='columns')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "f6FRpxH_iuS7",
    "outputId": "5f69878d-67c2-49c9-ce4c-c5925c92a63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 156 entries, 0 to 155\n",
      "Data columns (total 8 columns):\n",
      "Happiness_level                 156 non-null object\n",
      "GDP per capita                  156 non-null float64\n",
      "Social support                  156 non-null float64\n",
      "Healthy life expectancy         156 non-null float64\n",
      "Freedom to make life choices    156 non-null float64\n",
      "Generosity                      156 non-null float64\n",
      "Perceptions of corruption       156 non-null float64\n",
      "Country or region               134 non-null object\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 11.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsJSCsFyxFuV"
   },
   "source": [
    "#### 1. Explore bivariate results (Use visualizations!)\n",
    "##### 1.1 Describe any relationships you see between particular features and the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "JWT912b3_ZJ2",
    "outputId": "c3bf0953-f8e2-4afe-fb7b-bd0e06fbdc49"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAjOzJvffcBDFmmpCj7AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g= sns.boxplot(y=\"Perceptions of corruption\", x=\"Happiness_level\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "byBezE9vBCoz",
    "outputId": "3c334d58-2cad-4006-c0df-cbc41c93e1b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzArOicDMrOCcCMzMCu7/A8Az1KFx43dWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g= sns.boxplot(y=\"Generosity\", x=\"Happiness_level\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "7NhJebIyBJpl",
    "outputId": "b0bf2d9c-f600-427a-b80d-9745627fd178"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g= sns.boxplot(y=\"Freedom to make life choices\", x=\"Happiness_level\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "wjiaOqzWBPEo",
    "outputId": "a0cbcd39-986b-4bb9-9f81-e23c9e174a60"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g= sns.boxplot(y=\"Healthy life expectancy\", x=\"Happiness_level\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "2_gtlTp5BVXh",
    "outputId": "2002cff4-4140-46bd-bf62-6efaf5e5681e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVnBOBGZmBedEYGZWcE4EZmYF9/8BzcTZONztalwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g= sns.boxplot(y=\"Social support\", x=\"Happiness_level\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "mDfw1eo-BdAM",
    "outputId": "76216209-f7d9-4382-c14b-8fd108a85f53"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE4SZmeVygjAzs1z/H6qoPqbKWlBLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g= sns.boxplot(y=\"GDP per capita\", x=\"Happiness_level\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iddPw3VakKLY"
   },
   "source": [
    "When exploring the relationships between the target variable (*Happiness level*) and the particular features used in the UN calculations, it is shown that a positive correlation (and therefore likely influence) exists with *GDP per capita*, *Health expectancy*, and *Social support*. All of which are commonly considered to have colineality and depend on the income level of the population. Hence, these results are not surprising.\n",
    "\n",
    "However, the relationship between *Happiness* and *Generosity* is not as clear, given the variations and averages on each level of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDL_5V-txWzy"
   },
   "source": [
    "#### 2. Examine features that predict happiness categories using one or more models that allow for automatic feature selection\n",
    "##### 2.1 Explain any meaningful findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOTCVDE5ZR9R"
   },
   "source": [
    "### Divide train-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "bumaVl8WaS2g",
    "outputId": "f1b938e8-41b2-4e33-bbaf-6ec13351e3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 7)\n",
      "(117,)\n",
      "['GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Country or region']\n"
     ]
    }
   ],
   "source": [
    "y=data['Happiness_level']\n",
    "X=data.drop(['Happiness_level'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "DM518Y9gjYR0",
    "outputId": "6b6966b7-6fe5-4564-fdf4-4493c9d66f05"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "      <th>Country or region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GDP per capita  Social support  ...  Perceptions of corruption  Country or region\n",
       "0           1.340           1.587  ...                      0.393             Europe\n",
       "1           1.383           1.573  ...                      0.410             Europe\n",
       "2           1.488           1.582  ...                      0.341             Europe\n",
       "3           1.380           1.624  ...                      0.118             Europe\n",
       "4           1.396           1.522  ...                      0.298             Europe\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "bzsTv1S_jbOq",
    "outputId": "7d9eef3f-41c2-4aad-db51-a83bedac8592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Very High\n",
       "1    Very High\n",
       "2    Very High\n",
       "3    Very High\n",
       "4    Very High\n",
       "Name: Happiness_level, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Jy9fkMlyji9P",
    "outputId": "63426d69-26e8-4b95-8497-cc8601989e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 7)\n",
      "(156,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPqvykekjl0u"
   },
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "20AG7j8Ljnba"
   },
   "outputs": [],
   "source": [
    "numeric_features=X.columns.tolist()\n",
    "numeric_features.remove('Country or region')\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['Country or region']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73JFB-xvmWQL"
   },
   "outputs": [],
   "source": [
    "#Fit your preprocessor object\n",
    "prediction_input_preprocessor=preprocessor.fit(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Fl311gBldr0u",
    "outputId": "a713fa02-184f-4758-d581-01c5c6f26d2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape for keras input:\n",
    "prediction_input_preprocessor.transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4f2GeOkel5V5",
    "outputId": "833bb66d-bab5-4f07-956c-07cc92d29e21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape for keras output:\n",
    "pd.get_dummies(y_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H21wnUxWQVUN"
   },
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_yQMQ7scK-e"
   },
   "source": [
    "a) Random Forest Regressor + Select From Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "N9VJpHjecWr9",
    "outputId": "a3c9f10a-f330-40b8-8632-abe72bd4c208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22599904 0.15796512 0.24242157 0.13050417 0.07033921 0.12555213\n",
      " 0.00619142 0.02475552 0.01418841 0.00208342 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=200)\n",
    "formodel = forest.fit(prediction_input_preprocessor.transform(X_train), pd.get_dummies(y_train))\n",
    "\n",
    "\n",
    "print(formodel.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "FNoi2YILc4IX",
    "outputId": "4619641b-52b3-4134-8b63-f37a605b185a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_base.py:81: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "sfm = SelectFromModel(formodel, threshold=.25)\n",
    "sfm.fit(prediction_input_preprocessor.transform(X_train), pd.get_dummies(y_train))\n",
    "Xtrain_new = sfm.transform(prediction_input_preprocessor.transform(X_train)) # transform data to insert into new model\n",
    "\n",
    "print(prediction_input_preprocessor.transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWo7kxjoc7nR"
   },
   "source": [
    "In this case no variables are zereod, leading us to believe are variables are relevant to the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75kDw6gxLDmz"
   },
   "source": [
    "##### b) Lasso + Select from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "xg_T_7SsogZ5",
    "outputId": "d16bd4df-73ad-4a9b-e9cd-9247674b0eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lasso = LogisticRegression(penalty='l1', solver='saga')\n",
    "lasso.fit(prediction_input_preprocessor.transform(X_train), y_train)\n",
    "\n",
    "model = SelectFromModel(lasso, prefit=True)\n",
    "\n",
    "X_new = model.transform(prediction_input_preprocessor.transform(X_train))\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6WmzK4Mu0dPk",
    "outputId": "86c041c5-a694-4bf4-8ffa-86ccbfbb8c5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VE6QorgFd2ET"
   },
   "source": [
    "In contrast, to the Random Forest Classifier, the Lasso model zeroed 2 (*Generosity* and *Perception of corruption*) out of the 11 features. Leaving only 9 features, those relevant to the prediction of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJMZ71m1yFGn"
   },
   "source": [
    "#### 3. Run at least three prediction models to try to predict World Happiness well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFXxVyQ2yIeo"
   },
   "source": [
    "#### 3.1 Discuss which models performed better and point out relevant hyper-parameter values for successful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pGhNAVpB_Zc"
   },
   "source": [
    "Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OZ28BXzsB9Kp",
    "outputId": "0917429a-7f8f-4356-dffe-bb81430090ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 93 samples, validate on 24 samples\n",
      "Epoch 1/350\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 1.6037 - acc: 0.1720 - val_loss: 1.6451 - val_acc: 0.2500\n",
      "Epoch 2/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 1.6021 - acc: 0.1720 - val_loss: 1.6434 - val_acc: 0.2500\n",
      "Epoch 3/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.6001 - acc: 0.1720 - val_loss: 1.6420 - val_acc: 0.2500\n",
      "Epoch 4/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.5984 - acc: 0.1720 - val_loss: 1.6407 - val_acc: 0.2500\n",
      "Epoch 5/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 1.5967 - acc: 0.1720 - val_loss: 1.6397 - val_acc: 0.2500\n",
      "Epoch 6/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.5952 - acc: 0.1720 - val_loss: 1.6386 - val_acc: 0.2500\n",
      "Epoch 7/350\n",
      "93/93 [==============================] - 0s 60us/step - loss: 1.5936 - acc: 0.1828 - val_loss: 1.6370 - val_acc: 0.2500\n",
      "Epoch 8/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.5914 - acc: 0.1720 - val_loss: 1.6358 - val_acc: 0.2500\n",
      "Epoch 9/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.5898 - acc: 0.1613 - val_loss: 1.6348 - val_acc: 0.2500\n",
      "Epoch 10/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.5882 - acc: 0.1613 - val_loss: 1.6339 - val_acc: 0.2500\n",
      "Epoch 11/350\n",
      "93/93 [==============================] - 0s 61us/step - loss: 1.5866 - acc: 0.1613 - val_loss: 1.6329 - val_acc: 0.2500\n",
      "Epoch 12/350\n",
      "93/93 [==============================] - 0s 69us/step - loss: 1.5850 - acc: 0.1613 - val_loss: 1.6315 - val_acc: 0.2500\n",
      "Epoch 13/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.5835 - acc: 0.1613 - val_loss: 1.6308 - val_acc: 0.2500\n",
      "Epoch 14/350\n",
      "93/93 [==============================] - 0s 96us/step - loss: 1.5818 - acc: 0.1613 - val_loss: 1.6295 - val_acc: 0.2500\n",
      "Epoch 15/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.5802 - acc: 0.1613 - val_loss: 1.6283 - val_acc: 0.2500\n",
      "Epoch 16/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.5786 - acc: 0.1613 - val_loss: 1.6270 - val_acc: 0.2917\n",
      "Epoch 17/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.5769 - acc: 0.1935 - val_loss: 1.6259 - val_acc: 0.2500\n",
      "Epoch 18/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.5752 - acc: 0.2043 - val_loss: 1.6246 - val_acc: 0.2917\n",
      "Epoch 19/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.5737 - acc: 0.2043 - val_loss: 1.6233 - val_acc: 0.2917\n",
      "Epoch 20/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.5718 - acc: 0.2151 - val_loss: 1.6219 - val_acc: 0.3333\n",
      "Epoch 21/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.5701 - acc: 0.2366 - val_loss: 1.6205 - val_acc: 0.3333\n",
      "Epoch 22/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.5682 - acc: 0.2581 - val_loss: 1.6194 - val_acc: 0.3333\n",
      "Epoch 23/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.5667 - acc: 0.2688 - val_loss: 1.6180 - val_acc: 0.3333\n",
      "Epoch 24/350\n",
      "93/93 [==============================] - 0s 87us/step - loss: 1.5651 - acc: 0.2796 - val_loss: 1.6173 - val_acc: 0.3333\n",
      "Epoch 25/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 1.5630 - acc: 0.2796 - val_loss: 1.6159 - val_acc: 0.3333\n",
      "Epoch 26/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.5615 - acc: 0.2796 - val_loss: 1.6151 - val_acc: 0.3333\n",
      "Epoch 27/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 1.5599 - acc: 0.2796 - val_loss: 1.6138 - val_acc: 0.3333\n",
      "Epoch 28/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.5582 - acc: 0.2796 - val_loss: 1.6128 - val_acc: 0.3333\n",
      "Epoch 29/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.5566 - acc: 0.2903 - val_loss: 1.6118 - val_acc: 0.3333\n",
      "Epoch 30/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.5550 - acc: 0.2903 - val_loss: 1.6107 - val_acc: 0.3333\n",
      "Epoch 31/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.5538 - acc: 0.3011 - val_loss: 1.6099 - val_acc: 0.3333\n",
      "Epoch 32/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.5519 - acc: 0.3118 - val_loss: 1.6089 - val_acc: 0.3333\n",
      "Epoch 33/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.5502 - acc: 0.3333 - val_loss: 1.6079 - val_acc: 0.3333\n",
      "Epoch 34/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.5487 - acc: 0.3441 - val_loss: 1.6064 - val_acc: 0.3333\n",
      "Epoch 35/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.5469 - acc: 0.3441 - val_loss: 1.6053 - val_acc: 0.3333\n",
      "Epoch 36/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.5455 - acc: 0.3548 - val_loss: 1.6042 - val_acc: 0.3333\n",
      "Epoch 37/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.5437 - acc: 0.3548 - val_loss: 1.6029 - val_acc: 0.3750\n",
      "Epoch 38/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.5420 - acc: 0.3656 - val_loss: 1.6017 - val_acc: 0.3333\n",
      "Epoch 39/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 1.5403 - acc: 0.3763 - val_loss: 1.6006 - val_acc: 0.3333\n",
      "Epoch 40/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 1.5387 - acc: 0.3763 - val_loss: 1.5995 - val_acc: 0.3333\n",
      "Epoch 41/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 1.5370 - acc: 0.3871 - val_loss: 1.5985 - val_acc: 0.3333\n",
      "Epoch 42/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.5356 - acc: 0.3871 - val_loss: 1.5976 - val_acc: 0.3333\n",
      "Epoch 43/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.5339 - acc: 0.3978 - val_loss: 1.5963 - val_acc: 0.3333\n",
      "Epoch 44/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.5323 - acc: 0.3978 - val_loss: 1.5951 - val_acc: 0.3333\n",
      "Epoch 45/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.5307 - acc: 0.3978 - val_loss: 1.5940 - val_acc: 0.3333\n",
      "Epoch 46/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.5290 - acc: 0.3978 - val_loss: 1.5929 - val_acc: 0.3333\n",
      "Epoch 47/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.5273 - acc: 0.4086 - val_loss: 1.5919 - val_acc: 0.3333\n",
      "Epoch 48/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.5258 - acc: 0.4194 - val_loss: 1.5909 - val_acc: 0.3333\n",
      "Epoch 49/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 1.5244 - acc: 0.4086 - val_loss: 1.5899 - val_acc: 0.3333\n",
      "Epoch 50/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.5228 - acc: 0.4086 - val_loss: 1.5885 - val_acc: 0.3333\n",
      "Epoch 51/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.5211 - acc: 0.4194 - val_loss: 1.5877 - val_acc: 0.3333\n",
      "Epoch 52/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.5192 - acc: 0.4301 - val_loss: 1.5865 - val_acc: 0.3333\n",
      "Epoch 53/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.5176 - acc: 0.4301 - val_loss: 1.5849 - val_acc: 0.3333\n",
      "Epoch 54/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.5155 - acc: 0.4409 - val_loss: 1.5835 - val_acc: 0.3333\n",
      "Epoch 55/350\n",
      "93/93 [==============================] - 0s 87us/step - loss: 1.5140 - acc: 0.4409 - val_loss: 1.5827 - val_acc: 0.3333\n",
      "Epoch 56/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.5121 - acc: 0.4409 - val_loss: 1.5814 - val_acc: 0.3333\n",
      "Epoch 57/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.5103 - acc: 0.4409 - val_loss: 1.5801 - val_acc: 0.3333\n",
      "Epoch 58/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 1.5085 - acc: 0.4409 - val_loss: 1.5788 - val_acc: 0.3333\n",
      "Epoch 59/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.5067 - acc: 0.4516 - val_loss: 1.5774 - val_acc: 0.3750\n",
      "Epoch 60/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.5050 - acc: 0.4516 - val_loss: 1.5755 - val_acc: 0.3333\n",
      "Epoch 61/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.5028 - acc: 0.4516 - val_loss: 1.5739 - val_acc: 0.3333\n",
      "Epoch 62/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 1.5007 - acc: 0.4516 - val_loss: 1.5725 - val_acc: 0.3333\n",
      "Epoch 63/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.4990 - acc: 0.4516 - val_loss: 1.5714 - val_acc: 0.3333\n",
      "Epoch 64/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.4971 - acc: 0.4516 - val_loss: 1.5698 - val_acc: 0.3333\n",
      "Epoch 65/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.4952 - acc: 0.4516 - val_loss: 1.5683 - val_acc: 0.3333\n",
      "Epoch 66/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 1.4935 - acc: 0.4516 - val_loss: 1.5672 - val_acc: 0.3333\n",
      "Epoch 67/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.4916 - acc: 0.4624 - val_loss: 1.5654 - val_acc: 0.3333\n",
      "Epoch 68/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.4895 - acc: 0.4516 - val_loss: 1.5639 - val_acc: 0.3333\n",
      "Epoch 69/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.4875 - acc: 0.4516 - val_loss: 1.5625 - val_acc: 0.3333\n",
      "Epoch 70/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.4856 - acc: 0.4516 - val_loss: 1.5614 - val_acc: 0.3333\n",
      "Epoch 71/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.4838 - acc: 0.4516 - val_loss: 1.5605 - val_acc: 0.3333\n",
      "Epoch 72/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.4819 - acc: 0.4516 - val_loss: 1.5587 - val_acc: 0.3333\n",
      "Epoch 73/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.4797 - acc: 0.4409 - val_loss: 1.5571 - val_acc: 0.3333\n",
      "Epoch 74/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.4776 - acc: 0.4409 - val_loss: 1.5555 - val_acc: 0.3333\n",
      "Epoch 75/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 1.4758 - acc: 0.4301 - val_loss: 1.5546 - val_acc: 0.3333\n",
      "Epoch 76/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.4738 - acc: 0.4409 - val_loss: 1.5529 - val_acc: 0.3333\n",
      "Epoch 77/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.4720 - acc: 0.4409 - val_loss: 1.5512 - val_acc: 0.3333\n",
      "Epoch 78/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.4696 - acc: 0.4301 - val_loss: 1.5495 - val_acc: 0.3333\n",
      "Epoch 79/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.4674 - acc: 0.4409 - val_loss: 1.5483 - val_acc: 0.3333\n",
      "Epoch 80/350\n",
      "93/93 [==============================] - 0s 93us/step - loss: 1.4654 - acc: 0.4409 - val_loss: 1.5466 - val_acc: 0.3333\n",
      "Epoch 81/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 1.4632 - acc: 0.4409 - val_loss: 1.5450 - val_acc: 0.3333\n",
      "Epoch 82/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.4612 - acc: 0.4516 - val_loss: 1.5434 - val_acc: 0.3333\n",
      "Epoch 83/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.4590 - acc: 0.4516 - val_loss: 1.5419 - val_acc: 0.3333\n",
      "Epoch 84/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.4571 - acc: 0.4516 - val_loss: 1.5406 - val_acc: 0.3333\n",
      "Epoch 85/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.4550 - acc: 0.4409 - val_loss: 1.5394 - val_acc: 0.3333\n",
      "Epoch 86/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 1.4529 - acc: 0.4409 - val_loss: 1.5376 - val_acc: 0.3333\n",
      "Epoch 87/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.4507 - acc: 0.4516 - val_loss: 1.5361 - val_acc: 0.3333\n",
      "Epoch 88/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.4487 - acc: 0.4409 - val_loss: 1.5346 - val_acc: 0.3333\n",
      "Epoch 89/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.4466 - acc: 0.4409 - val_loss: 1.5331 - val_acc: 0.3333\n",
      "Epoch 90/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.4442 - acc: 0.4409 - val_loss: 1.5312 - val_acc: 0.3333\n",
      "Epoch 91/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.4419 - acc: 0.4516 - val_loss: 1.5296 - val_acc: 0.3333\n",
      "Epoch 92/350\n",
      "93/93 [==============================] - 0s 121us/step - loss: 1.4395 - acc: 0.4516 - val_loss: 1.5278 - val_acc: 0.3333\n",
      "Epoch 93/350\n",
      "93/93 [==============================] - 0s 102us/step - loss: 1.4371 - acc: 0.4624 - val_loss: 1.5262 - val_acc: 0.3333\n",
      "Epoch 94/350\n",
      "93/93 [==============================] - 0s 108us/step - loss: 1.4348 - acc: 0.4624 - val_loss: 1.5248 - val_acc: 0.3333\n",
      "Epoch 95/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.4326 - acc: 0.4624 - val_loss: 1.5230 - val_acc: 0.3333\n",
      "Epoch 96/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.4304 - acc: 0.4624 - val_loss: 1.5216 - val_acc: 0.3750\n",
      "Epoch 97/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.4282 - acc: 0.4624 - val_loss: 1.5194 - val_acc: 0.3750\n",
      "Epoch 98/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.4256 - acc: 0.4624 - val_loss: 1.5175 - val_acc: 0.3750\n",
      "Epoch 99/350\n",
      "93/93 [==============================] - 0s 94us/step - loss: 1.4233 - acc: 0.4624 - val_loss: 1.5159 - val_acc: 0.3750\n",
      "Epoch 100/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.4210 - acc: 0.4624 - val_loss: 1.5145 - val_acc: 0.3750\n",
      "Epoch 101/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.4188 - acc: 0.4624 - val_loss: 1.5132 - val_acc: 0.3750\n",
      "Epoch 102/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 1.4164 - acc: 0.4731 - val_loss: 1.5112 - val_acc: 0.3750\n",
      "Epoch 103/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.4140 - acc: 0.4731 - val_loss: 1.5094 - val_acc: 0.3750\n",
      "Epoch 104/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 1.4119 - acc: 0.4731 - val_loss: 1.5081 - val_acc: 0.3750\n",
      "Epoch 105/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.4092 - acc: 0.4731 - val_loss: 1.5063 - val_acc: 0.3750\n",
      "Epoch 106/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.4069 - acc: 0.4731 - val_loss: 1.5042 - val_acc: 0.3750\n",
      "Epoch 107/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.4044 - acc: 0.4731 - val_loss: 1.5017 - val_acc: 0.3750\n",
      "Epoch 108/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.4017 - acc: 0.4624 - val_loss: 1.4997 - val_acc: 0.3750\n",
      "Epoch 109/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.3991 - acc: 0.4624 - val_loss: 1.4977 - val_acc: 0.4167\n",
      "Epoch 110/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.3965 - acc: 0.4624 - val_loss: 1.4956 - val_acc: 0.4167\n",
      "Epoch 111/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 1.3940 - acc: 0.4624 - val_loss: 1.4938 - val_acc: 0.4167\n",
      "Epoch 112/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.3916 - acc: 0.4731 - val_loss: 1.4921 - val_acc: 0.4167\n",
      "Epoch 113/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 1.3892 - acc: 0.4731 - val_loss: 1.4903 - val_acc: 0.4167\n",
      "Epoch 114/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.3867 - acc: 0.4731 - val_loss: 1.4881 - val_acc: 0.4167\n",
      "Epoch 115/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 1.3840 - acc: 0.4731 - val_loss: 1.4864 - val_acc: 0.4167\n",
      "Epoch 116/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 1.3813 - acc: 0.4731 - val_loss: 1.4848 - val_acc: 0.4167\n",
      "Epoch 117/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.3788 - acc: 0.4731 - val_loss: 1.4832 - val_acc: 0.4167\n",
      "Epoch 118/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 1.3764 - acc: 0.4731 - val_loss: 1.4818 - val_acc: 0.4167\n",
      "Epoch 119/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 1.3739 - acc: 0.4731 - val_loss: 1.4805 - val_acc: 0.4167\n",
      "Epoch 120/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.3717 - acc: 0.4731 - val_loss: 1.4786 - val_acc: 0.4167\n",
      "Epoch 121/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.3687 - acc: 0.4731 - val_loss: 1.4768 - val_acc: 0.4167\n",
      "Epoch 122/350\n",
      "93/93 [==============================] - 0s 91us/step - loss: 1.3661 - acc: 0.4731 - val_loss: 1.4754 - val_acc: 0.4167\n",
      "Epoch 123/350\n",
      "93/93 [==============================] - 0s 91us/step - loss: 1.3638 - acc: 0.4731 - val_loss: 1.4735 - val_acc: 0.4167\n",
      "Epoch 124/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.3609 - acc: 0.4946 - val_loss: 1.4720 - val_acc: 0.4167\n",
      "Epoch 125/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.3585 - acc: 0.4839 - val_loss: 1.4704 - val_acc: 0.4167\n",
      "Epoch 126/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.3556 - acc: 0.4839 - val_loss: 1.4682 - val_acc: 0.4167\n",
      "Epoch 127/350\n",
      "93/93 [==============================] - 0s 93us/step - loss: 1.3530 - acc: 0.4839 - val_loss: 1.4665 - val_acc: 0.4167\n",
      "Epoch 128/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.3502 - acc: 0.4839 - val_loss: 1.4642 - val_acc: 0.4167\n",
      "Epoch 129/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.3476 - acc: 0.4839 - val_loss: 1.4630 - val_acc: 0.4167\n",
      "Epoch 130/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.3448 - acc: 0.4946 - val_loss: 1.4615 - val_acc: 0.4167\n",
      "Epoch 131/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.3423 - acc: 0.4946 - val_loss: 1.4600 - val_acc: 0.4167\n",
      "Epoch 132/350\n",
      "93/93 [==============================] - 0s 93us/step - loss: 1.3397 - acc: 0.4946 - val_loss: 1.4583 - val_acc: 0.3750\n",
      "Epoch 133/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.3372 - acc: 0.4946 - val_loss: 1.4567 - val_acc: 0.3750\n",
      "Epoch 134/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.3347 - acc: 0.5054 - val_loss: 1.4549 - val_acc: 0.3750\n",
      "Epoch 135/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.3321 - acc: 0.5054 - val_loss: 1.4534 - val_acc: 0.3750\n",
      "Epoch 136/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.3298 - acc: 0.4946 - val_loss: 1.4525 - val_acc: 0.3750\n",
      "Epoch 137/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 1.3271 - acc: 0.5054 - val_loss: 1.4504 - val_acc: 0.3750\n",
      "Epoch 138/350\n",
      "93/93 [==============================] - 0s 98us/step - loss: 1.3244 - acc: 0.5054 - val_loss: 1.4481 - val_acc: 0.3750\n",
      "Epoch 139/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.3216 - acc: 0.4946 - val_loss: 1.4463 - val_acc: 0.3750\n",
      "Epoch 140/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.3189 - acc: 0.4946 - val_loss: 1.4443 - val_acc: 0.3750\n",
      "Epoch 141/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.3160 - acc: 0.5054 - val_loss: 1.4429 - val_acc: 0.3750\n",
      "Epoch 142/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.3131 - acc: 0.5054 - val_loss: 1.4408 - val_acc: 0.3750\n",
      "Epoch 143/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.3104 - acc: 0.4946 - val_loss: 1.4385 - val_acc: 0.3750\n",
      "Epoch 144/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 1.3076 - acc: 0.5054 - val_loss: 1.4370 - val_acc: 0.3750\n",
      "Epoch 145/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.3048 - acc: 0.4946 - val_loss: 1.4344 - val_acc: 0.3750\n",
      "Epoch 146/350\n",
      "93/93 [==============================] - 0s 102us/step - loss: 1.3019 - acc: 0.4946 - val_loss: 1.4330 - val_acc: 0.3750\n",
      "Epoch 147/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.2990 - acc: 0.4946 - val_loss: 1.4310 - val_acc: 0.3750\n",
      "Epoch 148/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.2965 - acc: 0.5054 - val_loss: 1.4298 - val_acc: 0.3750\n",
      "Epoch 149/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.2937 - acc: 0.4946 - val_loss: 1.4283 - val_acc: 0.3750\n",
      "Epoch 150/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.2910 - acc: 0.4946 - val_loss: 1.4262 - val_acc: 0.3750\n",
      "Epoch 151/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.2882 - acc: 0.5161 - val_loss: 1.4245 - val_acc: 0.3750\n",
      "Epoch 152/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.2853 - acc: 0.5161 - val_loss: 1.4225 - val_acc: 0.3750\n",
      "Epoch 153/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.2825 - acc: 0.5161 - val_loss: 1.4210 - val_acc: 0.3750\n",
      "Epoch 154/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.2799 - acc: 0.5161 - val_loss: 1.4181 - val_acc: 0.3750\n",
      "Epoch 155/350\n",
      "93/93 [==============================] - 0s 91us/step - loss: 1.2770 - acc: 0.5269 - val_loss: 1.4166 - val_acc: 0.3750\n",
      "Epoch 156/350\n",
      "93/93 [==============================] - 0s 93us/step - loss: 1.2742 - acc: 0.5269 - val_loss: 1.4150 - val_acc: 0.3750\n",
      "Epoch 157/350\n",
      "93/93 [==============================] - 0s 87us/step - loss: 1.2716 - acc: 0.5269 - val_loss: 1.4135 - val_acc: 0.3750\n",
      "Epoch 158/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.2688 - acc: 0.5161 - val_loss: 1.4119 - val_acc: 0.3750\n",
      "Epoch 159/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 1.2662 - acc: 0.5161 - val_loss: 1.4100 - val_acc: 0.3750\n",
      "Epoch 160/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 1.2635 - acc: 0.4946 - val_loss: 1.4084 - val_acc: 0.3750\n",
      "Epoch 161/350\n",
      "93/93 [==============================] - 0s 102us/step - loss: 1.2610 - acc: 0.4839 - val_loss: 1.4068 - val_acc: 0.3750\n",
      "Epoch 162/350\n",
      "93/93 [==============================] - 0s 101us/step - loss: 1.2582 - acc: 0.5054 - val_loss: 1.4053 - val_acc: 0.3750\n",
      "Epoch 163/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.2557 - acc: 0.5054 - val_loss: 1.4031 - val_acc: 0.3750\n",
      "Epoch 164/350\n",
      "93/93 [==============================] - 0s 107us/step - loss: 1.2532 - acc: 0.5161 - val_loss: 1.4023 - val_acc: 0.3750\n",
      "Epoch 165/350\n",
      "93/93 [==============================] - 0s 96us/step - loss: 1.2505 - acc: 0.5161 - val_loss: 1.4006 - val_acc: 0.3750\n",
      "Epoch 166/350\n",
      "93/93 [==============================] - 0s 98us/step - loss: 1.2484 - acc: 0.5269 - val_loss: 1.4002 - val_acc: 0.3750\n",
      "Epoch 167/350\n",
      "93/93 [==============================] - 0s 97us/step - loss: 1.2457 - acc: 0.5269 - val_loss: 1.3983 - val_acc: 0.3750\n",
      "Epoch 168/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.2431 - acc: 0.5269 - val_loss: 1.3972 - val_acc: 0.3750\n",
      "Epoch 169/350\n",
      "93/93 [==============================] - 0s 93us/step - loss: 1.2406 - acc: 0.5269 - val_loss: 1.3956 - val_acc: 0.3750\n",
      "Epoch 170/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.2381 - acc: 0.5269 - val_loss: 1.3946 - val_acc: 0.3750\n",
      "Epoch 171/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.2358 - acc: 0.5269 - val_loss: 1.3935 - val_acc: 0.3750\n",
      "Epoch 172/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.2333 - acc: 0.5269 - val_loss: 1.3911 - val_acc: 0.3750\n",
      "Epoch 173/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.2308 - acc: 0.5269 - val_loss: 1.3900 - val_acc: 0.3750\n",
      "Epoch 174/350\n",
      "93/93 [==============================] - 0s 96us/step - loss: 1.2283 - acc: 0.5269 - val_loss: 1.3882 - val_acc: 0.3750\n",
      "Epoch 175/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 1.2258 - acc: 0.5269 - val_loss: 1.3868 - val_acc: 0.3750\n",
      "Epoch 176/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.2233 - acc: 0.5269 - val_loss: 1.3857 - val_acc: 0.3750\n",
      "Epoch 177/350\n",
      "93/93 [==============================] - 0s 94us/step - loss: 1.2208 - acc: 0.5269 - val_loss: 1.3846 - val_acc: 0.3750\n",
      "Epoch 178/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.2183 - acc: 0.5269 - val_loss: 1.3833 - val_acc: 0.3750\n",
      "Epoch 179/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.2163 - acc: 0.5269 - val_loss: 1.3823 - val_acc: 0.3750\n",
      "Epoch 180/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.2137 - acc: 0.5376 - val_loss: 1.3808 - val_acc: 0.3750\n",
      "Epoch 181/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.2112 - acc: 0.5376 - val_loss: 1.3786 - val_acc: 0.3750\n",
      "Epoch 182/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.2087 - acc: 0.5376 - val_loss: 1.3772 - val_acc: 0.3750\n",
      "Epoch 183/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.2062 - acc: 0.5376 - val_loss: 1.3758 - val_acc: 0.3750\n",
      "Epoch 184/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.2037 - acc: 0.5484 - val_loss: 1.3744 - val_acc: 0.3750\n",
      "Epoch 185/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.2013 - acc: 0.5376 - val_loss: 1.3722 - val_acc: 0.3750\n",
      "Epoch 186/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 1.1986 - acc: 0.5376 - val_loss: 1.3705 - val_acc: 0.3750\n",
      "Epoch 187/350\n",
      "93/93 [==============================] - 0s 98us/step - loss: 1.1965 - acc: 0.5269 - val_loss: 1.3693 - val_acc: 0.3750\n",
      "Epoch 188/350\n",
      "93/93 [==============================] - 0s 99us/step - loss: 1.1941 - acc: 0.5376 - val_loss: 1.3685 - val_acc: 0.3750\n",
      "Epoch 189/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.1918 - acc: 0.5376 - val_loss: 1.3678 - val_acc: 0.3750\n",
      "Epoch 190/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.1899 - acc: 0.5376 - val_loss: 1.3662 - val_acc: 0.3750\n",
      "Epoch 191/350\n",
      "93/93 [==============================] - 0s 103us/step - loss: 1.1872 - acc: 0.5376 - val_loss: 1.3651 - val_acc: 0.3750\n",
      "Epoch 192/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 1.1848 - acc: 0.5484 - val_loss: 1.3638 - val_acc: 0.3750\n",
      "Epoch 193/350\n",
      "93/93 [==============================] - 0s 107us/step - loss: 1.1829 - acc: 0.5376 - val_loss: 1.3626 - val_acc: 0.3750\n",
      "Epoch 194/350\n",
      "93/93 [==============================] - 0s 100us/step - loss: 1.1802 - acc: 0.5376 - val_loss: 1.3609 - val_acc: 0.3750\n",
      "Epoch 195/350\n",
      "93/93 [==============================] - 0s 97us/step - loss: 1.1779 - acc: 0.5484 - val_loss: 1.3597 - val_acc: 0.3750\n",
      "Epoch 196/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 1.1761 - acc: 0.5484 - val_loss: 1.3594 - val_acc: 0.3750\n",
      "Epoch 197/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.1734 - acc: 0.5484 - val_loss: 1.3582 - val_acc: 0.3750\n",
      "Epoch 198/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.1713 - acc: 0.5484 - val_loss: 1.3569 - val_acc: 0.3750\n",
      "Epoch 199/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 1.1691 - acc: 0.5484 - val_loss: 1.3564 - val_acc: 0.3750\n",
      "Epoch 200/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.1669 - acc: 0.5484 - val_loss: 1.3551 - val_acc: 0.3750\n",
      "Epoch 201/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 1.1647 - acc: 0.5484 - val_loss: 1.3542 - val_acc: 0.3750\n",
      "Epoch 202/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.1625 - acc: 0.5484 - val_loss: 1.3531 - val_acc: 0.3750\n",
      "Epoch 203/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.1606 - acc: 0.5484 - val_loss: 1.3524 - val_acc: 0.3750\n",
      "Epoch 204/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.1581 - acc: 0.5484 - val_loss: 1.3516 - val_acc: 0.3750\n",
      "Epoch 205/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.1560 - acc: 0.5484 - val_loss: 1.3502 - val_acc: 0.3750\n",
      "Epoch 206/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.1536 - acc: 0.5484 - val_loss: 1.3493 - val_acc: 0.3750\n",
      "Epoch 207/350\n",
      "93/93 [==============================] - 0s 91us/step - loss: 1.1516 - acc: 0.5484 - val_loss: 1.3484 - val_acc: 0.3750\n",
      "Epoch 208/350\n",
      "93/93 [==============================] - 0s 94us/step - loss: 1.1496 - acc: 0.5376 - val_loss: 1.3473 - val_acc: 0.3750\n",
      "Epoch 209/350\n",
      "93/93 [==============================] - 0s 63us/step - loss: 1.1473 - acc: 0.5484 - val_loss: 1.3465 - val_acc: 0.3750\n",
      "Epoch 210/350\n",
      "93/93 [==============================] - 0s 62us/step - loss: 1.1453 - acc: 0.5699 - val_loss: 1.3456 - val_acc: 0.3750\n",
      "Epoch 211/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.1433 - acc: 0.5591 - val_loss: 1.3440 - val_acc: 0.3750\n",
      "Epoch 212/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.1410 - acc: 0.5699 - val_loss: 1.3426 - val_acc: 0.3750\n",
      "Epoch 213/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.1389 - acc: 0.5591 - val_loss: 1.3412 - val_acc: 0.3750\n",
      "Epoch 214/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.1368 - acc: 0.5591 - val_loss: 1.3397 - val_acc: 0.3750\n",
      "Epoch 215/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.1348 - acc: 0.5591 - val_loss: 1.3387 - val_acc: 0.3750\n",
      "Epoch 216/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.1328 - acc: 0.5591 - val_loss: 1.3375 - val_acc: 0.3750\n",
      "Epoch 217/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.1308 - acc: 0.5806 - val_loss: 1.3370 - val_acc: 0.3750\n",
      "Epoch 218/350\n",
      "93/93 [==============================] - 0s 103us/step - loss: 1.1289 - acc: 0.5699 - val_loss: 1.3350 - val_acc: 0.3750\n",
      "Epoch 219/350\n",
      "93/93 [==============================] - 0s 61us/step - loss: 1.1270 - acc: 0.5699 - val_loss: 1.3346 - val_acc: 0.3750\n",
      "Epoch 220/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.1254 - acc: 0.5699 - val_loss: 1.3341 - val_acc: 0.3750\n",
      "Epoch 221/350\n",
      "93/93 [==============================] - 0s 114us/step - loss: 1.1232 - acc: 0.5591 - val_loss: 1.3327 - val_acc: 0.3750\n",
      "Epoch 222/350\n",
      "93/93 [==============================] - 0s 132us/step - loss: 1.1218 - acc: 0.5699 - val_loss: 1.3326 - val_acc: 0.3750\n",
      "Epoch 223/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.1195 - acc: 0.5699 - val_loss: 1.3326 - val_acc: 0.3750\n",
      "Epoch 224/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.1176 - acc: 0.5699 - val_loss: 1.3315 - val_acc: 0.3750\n",
      "Epoch 225/350\n",
      "93/93 [==============================] - 0s 139us/step - loss: 1.1157 - acc: 0.5699 - val_loss: 1.3310 - val_acc: 0.3750\n",
      "Epoch 226/350\n",
      "93/93 [==============================] - 0s 150us/step - loss: 1.1140 - acc: 0.5699 - val_loss: 1.3306 - val_acc: 0.3750\n",
      "Epoch 227/350\n",
      "93/93 [==============================] - 0s 122us/step - loss: 1.1124 - acc: 0.5699 - val_loss: 1.3307 - val_acc: 0.3750\n",
      "Epoch 228/350\n",
      "93/93 [==============================] - 0s 137us/step - loss: 1.1103 - acc: 0.5591 - val_loss: 1.3302 - val_acc: 0.3333\n",
      "Epoch 229/350\n",
      "93/93 [==============================] - 0s 133us/step - loss: 1.1087 - acc: 0.5699 - val_loss: 1.3301 - val_acc: 0.3333\n",
      "Epoch 230/350\n",
      "93/93 [==============================] - 0s 149us/step - loss: 1.1070 - acc: 0.5591 - val_loss: 1.3302 - val_acc: 0.3333\n",
      "Epoch 231/350\n",
      "93/93 [==============================] - 0s 112us/step - loss: 1.1052 - acc: 0.5699 - val_loss: 1.3289 - val_acc: 0.3333\n",
      "Epoch 232/350\n",
      "93/93 [==============================] - 0s 130us/step - loss: 1.1033 - acc: 0.5591 - val_loss: 1.3272 - val_acc: 0.3750\n",
      "Epoch 233/350\n",
      "93/93 [==============================] - 0s 126us/step - loss: 1.1011 - acc: 0.5591 - val_loss: 1.3271 - val_acc: 0.3750\n",
      "Epoch 234/350\n",
      "93/93 [==============================] - 0s 105us/step - loss: 1.0996 - acc: 0.5806 - val_loss: 1.3256 - val_acc: 0.3750\n",
      "Epoch 235/350\n",
      "93/93 [==============================] - 0s 108us/step - loss: 1.0974 - acc: 0.5699 - val_loss: 1.3243 - val_acc: 0.3750\n",
      "Epoch 236/350\n",
      "93/93 [==============================] - 0s 124us/step - loss: 1.0956 - acc: 0.5806 - val_loss: 1.3227 - val_acc: 0.3750\n",
      "Epoch 237/350\n",
      "93/93 [==============================] - 0s 112us/step - loss: 1.0934 - acc: 0.5806 - val_loss: 1.3213 - val_acc: 0.3750\n",
      "Epoch 238/350\n",
      "93/93 [==============================] - 0s 130us/step - loss: 1.0914 - acc: 0.5806 - val_loss: 1.3203 - val_acc: 0.3750\n",
      "Epoch 239/350\n",
      "93/93 [==============================] - 0s 136us/step - loss: 1.0899 - acc: 0.5806 - val_loss: 1.3199 - val_acc: 0.4167\n",
      "Epoch 240/350\n",
      "93/93 [==============================] - 0s 127us/step - loss: 1.0877 - acc: 0.5806 - val_loss: 1.3193 - val_acc: 0.4167\n",
      "Epoch 241/350\n",
      "93/93 [==============================] - 0s 148us/step - loss: 1.0861 - acc: 0.5806 - val_loss: 1.3187 - val_acc: 0.3750\n",
      "Epoch 242/350\n",
      "93/93 [==============================] - 0s 155us/step - loss: 1.0846 - acc: 0.5806 - val_loss: 1.3182 - val_acc: 0.4167\n",
      "Epoch 243/350\n",
      "93/93 [==============================] - 0s 154us/step - loss: 1.0825 - acc: 0.5806 - val_loss: 1.3174 - val_acc: 0.4167\n",
      "Epoch 244/350\n",
      "93/93 [==============================] - 0s 125us/step - loss: 1.0809 - acc: 0.5806 - val_loss: 1.3177 - val_acc: 0.3750\n",
      "Epoch 245/350\n",
      "93/93 [==============================] - 0s 153us/step - loss: 1.0790 - acc: 0.5806 - val_loss: 1.3174 - val_acc: 0.4167\n",
      "Epoch 246/350\n",
      "93/93 [==============================] - 0s 124us/step - loss: 1.0774 - acc: 0.5806 - val_loss: 1.3177 - val_acc: 0.4167\n",
      "Epoch 247/350\n",
      "93/93 [==============================] - 0s 103us/step - loss: 1.0757 - acc: 0.5806 - val_loss: 1.3158 - val_acc: 0.4167\n",
      "Epoch 248/350\n",
      "93/93 [==============================] - 0s 120us/step - loss: 1.0741 - acc: 0.5806 - val_loss: 1.3146 - val_acc: 0.4167\n",
      "Epoch 249/350\n",
      "93/93 [==============================] - 0s 142us/step - loss: 1.0722 - acc: 0.5806 - val_loss: 1.3147 - val_acc: 0.4167\n",
      "Epoch 250/350\n",
      "93/93 [==============================] - 0s 134us/step - loss: 1.0702 - acc: 0.5806 - val_loss: 1.3142 - val_acc: 0.4167\n",
      "Epoch 251/350\n",
      "93/93 [==============================] - 0s 91us/step - loss: 1.0686 - acc: 0.5806 - val_loss: 1.3147 - val_acc: 0.4167\n",
      "Epoch 252/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 1.0670 - acc: 0.5806 - val_loss: 1.3136 - val_acc: 0.4167\n",
      "Epoch 253/350\n",
      "93/93 [==============================] - 0s 115us/step - loss: 1.0652 - acc: 0.5806 - val_loss: 1.3135 - val_acc: 0.4167\n",
      "Epoch 254/350\n",
      "93/93 [==============================] - 0s 145us/step - loss: 1.0644 - acc: 0.5806 - val_loss: 1.3126 - val_acc: 0.4167\n",
      "Epoch 255/350\n",
      "93/93 [==============================] - 0s 87us/step - loss: 1.0619 - acc: 0.5806 - val_loss: 1.3115 - val_acc: 0.4167\n",
      "Epoch 256/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.0615 - acc: 0.5806 - val_loss: 1.3117 - val_acc: 0.4583\n",
      "Epoch 257/350\n",
      "93/93 [==============================] - 0s 92us/step - loss: 1.0587 - acc: 0.5914 - val_loss: 1.3120 - val_acc: 0.4167\n",
      "Epoch 258/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.0573 - acc: 0.5806 - val_loss: 1.3110 - val_acc: 0.4167\n",
      "Epoch 259/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.0549 - acc: 0.5806 - val_loss: 1.3102 - val_acc: 0.4167\n",
      "Epoch 260/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 1.0533 - acc: 0.5806 - val_loss: 1.3101 - val_acc: 0.4167\n",
      "Epoch 261/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 1.0519 - acc: 0.5806 - val_loss: 1.3107 - val_acc: 0.4167\n",
      "Epoch 262/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.0498 - acc: 0.5806 - val_loss: 1.3102 - val_acc: 0.4167\n",
      "Epoch 263/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.0485 - acc: 0.5806 - val_loss: 1.3100 - val_acc: 0.4167\n",
      "Epoch 264/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.0470 - acc: 0.5806 - val_loss: 1.3092 - val_acc: 0.4167\n",
      "Epoch 265/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.0454 - acc: 0.5806 - val_loss: 1.3100 - val_acc: 0.4167\n",
      "Epoch 266/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.0435 - acc: 0.5806 - val_loss: 1.3091 - val_acc: 0.4167\n",
      "Epoch 267/350\n",
      "93/93 [==============================] - 0s 139us/step - loss: 1.0421 - acc: 0.5806 - val_loss: 1.3075 - val_acc: 0.4167\n",
      "Epoch 268/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.0397 - acc: 0.5806 - val_loss: 1.3069 - val_acc: 0.4167\n",
      "Epoch 269/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.0386 - acc: 0.5806 - val_loss: 1.3074 - val_acc: 0.4167\n",
      "Epoch 270/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 1.0366 - acc: 0.5806 - val_loss: 1.3079 - val_acc: 0.4167\n",
      "Epoch 271/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 1.0352 - acc: 0.5806 - val_loss: 1.3079 - val_acc: 0.4167\n",
      "Epoch 272/350\n",
      "93/93 [==============================] - 0s 86us/step - loss: 1.0335 - acc: 0.5806 - val_loss: 1.3077 - val_acc: 0.4167\n",
      "Epoch 273/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.0320 - acc: 0.5806 - val_loss: 1.3070 - val_acc: 0.4167\n",
      "Epoch 274/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 1.0307 - acc: 0.5806 - val_loss: 1.3062 - val_acc: 0.4167\n",
      "Epoch 275/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.0290 - acc: 0.5806 - val_loss: 1.3058 - val_acc: 0.4167\n",
      "Epoch 276/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 1.0275 - acc: 0.5806 - val_loss: 1.3049 - val_acc: 0.4167\n",
      "Epoch 277/350\n",
      "93/93 [==============================] - 0s 69us/step - loss: 1.0259 - acc: 0.5806 - val_loss: 1.3045 - val_acc: 0.4167\n",
      "Epoch 278/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 1.0243 - acc: 0.5806 - val_loss: 1.3046 - val_acc: 0.4167\n",
      "Epoch 279/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 1.0227 - acc: 0.5806 - val_loss: 1.3041 - val_acc: 0.4167\n",
      "Epoch 280/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.0210 - acc: 0.5806 - val_loss: 1.3035 - val_acc: 0.4167\n",
      "Epoch 281/350\n",
      "93/93 [==============================] - 0s 88us/step - loss: 1.0195 - acc: 0.5806 - val_loss: 1.3027 - val_acc: 0.4167\n",
      "Epoch 282/350\n",
      "93/93 [==============================] - 0s 87us/step - loss: 1.0184 - acc: 0.5806 - val_loss: 1.3016 - val_acc: 0.4167\n",
      "Epoch 283/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.0167 - acc: 0.5806 - val_loss: 1.3007 - val_acc: 0.4167\n",
      "Epoch 284/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 1.0155 - acc: 0.5914 - val_loss: 1.3012 - val_acc: 0.4167\n",
      "Epoch 285/350\n",
      "93/93 [==============================] - 0s 147us/step - loss: 1.0142 - acc: 0.5806 - val_loss: 1.3021 - val_acc: 0.4167\n",
      "Epoch 286/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 1.0127 - acc: 0.5806 - val_loss: 1.3028 - val_acc: 0.4167\n",
      "Epoch 287/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 1.0114 - acc: 0.5806 - val_loss: 1.3018 - val_acc: 0.4167\n",
      "Epoch 288/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 1.0100 - acc: 0.5914 - val_loss: 1.3014 - val_acc: 0.4167\n",
      "Epoch 289/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 1.0091 - acc: 0.5914 - val_loss: 1.3017 - val_acc: 0.4167\n",
      "Epoch 290/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 1.0073 - acc: 0.5914 - val_loss: 1.3006 - val_acc: 0.4167\n",
      "Epoch 291/350\n",
      "93/93 [==============================] - 0s 89us/step - loss: 1.0057 - acc: 0.5914 - val_loss: 1.3000 - val_acc: 0.4167\n",
      "Epoch 292/350\n",
      "93/93 [==============================] - 0s 83us/step - loss: 1.0045 - acc: 0.5914 - val_loss: 1.2998 - val_acc: 0.4167\n",
      "Epoch 293/350\n",
      "93/93 [==============================] - 0s 81us/step - loss: 1.0038 - acc: 0.5914 - val_loss: 1.2998 - val_acc: 0.4583\n",
      "Epoch 294/350\n",
      "93/93 [==============================] - 0s 65us/step - loss: 1.0024 - acc: 0.6129 - val_loss: 1.2997 - val_acc: 0.5000\n",
      "Epoch 295/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 1.0004 - acc: 0.6129 - val_loss: 1.2996 - val_acc: 0.5000\n",
      "Epoch 296/350\n",
      "93/93 [==============================] - 0s 62us/step - loss: 0.9997 - acc: 0.6129 - val_loss: 1.3012 - val_acc: 0.4583\n",
      "Epoch 297/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9976 - acc: 0.6129 - val_loss: 1.3015 - val_acc: 0.4583\n",
      "Epoch 298/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 0.9965 - acc: 0.6129 - val_loss: 1.3020 - val_acc: 0.4583\n",
      "Epoch 299/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 0.9954 - acc: 0.6129 - val_loss: 1.3013 - val_acc: 0.4583\n",
      "Epoch 300/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9939 - acc: 0.6129 - val_loss: 1.3014 - val_acc: 0.5000\n",
      "Epoch 301/350\n",
      "93/93 [==============================] - 0s 75us/step - loss: 0.9927 - acc: 0.6129 - val_loss: 1.3023 - val_acc: 0.5000\n",
      "Epoch 302/350\n",
      "93/93 [==============================] - 0s 144us/step - loss: 0.9912 - acc: 0.6129 - val_loss: 1.3028 - val_acc: 0.5000\n",
      "Epoch 303/350\n",
      "93/93 [==============================] - 0s 153us/step - loss: 0.9901 - acc: 0.6129 - val_loss: 1.3038 - val_acc: 0.5417\n",
      "Epoch 304/350\n",
      "93/93 [==============================] - 0s 102us/step - loss: 0.9889 - acc: 0.6129 - val_loss: 1.3028 - val_acc: 0.5417\n",
      "Epoch 305/350\n",
      "93/93 [==============================] - 0s 85us/step - loss: 0.9879 - acc: 0.6237 - val_loss: 1.3017 - val_acc: 0.5417\n",
      "Epoch 306/350\n",
      "93/93 [==============================] - 0s 99us/step - loss: 0.9860 - acc: 0.6237 - val_loss: 1.3018 - val_acc: 0.5417\n",
      "Epoch 307/350\n",
      "93/93 [==============================] - 0s 116us/step - loss: 0.9850 - acc: 0.6237 - val_loss: 1.3016 - val_acc: 0.5000\n",
      "Epoch 308/350\n",
      "93/93 [==============================] - 0s 95us/step - loss: 0.9840 - acc: 0.6129 - val_loss: 1.3020 - val_acc: 0.5417\n",
      "Epoch 309/350\n",
      "93/93 [==============================] - 0s 90us/step - loss: 0.9833 - acc: 0.6129 - val_loss: 1.3018 - val_acc: 0.5417\n",
      "Epoch 310/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 0.9819 - acc: 0.6237 - val_loss: 1.3019 - val_acc: 0.5417\n",
      "Epoch 311/350\n",
      "93/93 [==============================] - 0s 69us/step - loss: 0.9804 - acc: 0.6237 - val_loss: 1.3004 - val_acc: 0.5417\n",
      "Epoch 312/350\n",
      "93/93 [==============================] - 0s 62us/step - loss: 0.9799 - acc: 0.6237 - val_loss: 1.3014 - val_acc: 0.5417\n",
      "Epoch 313/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9788 - acc: 0.6237 - val_loss: 1.3009 - val_acc: 0.5417\n",
      "Epoch 314/350\n",
      "93/93 [==============================] - 0s 63us/step - loss: 0.9767 - acc: 0.6237 - val_loss: 1.3013 - val_acc: 0.5417\n",
      "Epoch 315/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 0.9757 - acc: 0.6237 - val_loss: 1.3018 - val_acc: 0.5417\n",
      "Epoch 316/350\n",
      "93/93 [==============================] - 0s 65us/step - loss: 0.9751 - acc: 0.6237 - val_loss: 1.3014 - val_acc: 0.5417\n",
      "Epoch 317/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 0.9734 - acc: 0.6237 - val_loss: 1.3014 - val_acc: 0.5417\n",
      "Epoch 318/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9727 - acc: 0.6237 - val_loss: 1.3012 - val_acc: 0.5417\n",
      "Epoch 319/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 0.9718 - acc: 0.6237 - val_loss: 1.3019 - val_acc: 0.5417\n",
      "Epoch 320/350\n",
      "93/93 [==============================] - 0s 69us/step - loss: 0.9698 - acc: 0.6237 - val_loss: 1.3017 - val_acc: 0.5417\n",
      "Epoch 321/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 0.9683 - acc: 0.6237 - val_loss: 1.3017 - val_acc: 0.5417\n",
      "Epoch 322/350\n",
      "93/93 [==============================] - 0s 84us/step - loss: 0.9670 - acc: 0.6237 - val_loss: 1.3017 - val_acc: 0.5417\n",
      "Epoch 323/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 0.9663 - acc: 0.6237 - val_loss: 1.3029 - val_acc: 0.5417\n",
      "Epoch 324/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 0.9651 - acc: 0.6237 - val_loss: 1.3035 - val_acc: 0.5417\n",
      "Epoch 325/350\n",
      "93/93 [==============================] - 0s 79us/step - loss: 0.9648 - acc: 0.6344 - val_loss: 1.3027 - val_acc: 0.5417\n",
      "Epoch 326/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 0.9628 - acc: 0.6237 - val_loss: 1.3021 - val_acc: 0.5417\n",
      "Epoch 327/350\n",
      "93/93 [==============================] - 0s 69us/step - loss: 0.9615 - acc: 0.6237 - val_loss: 1.3020 - val_acc: 0.5417\n",
      "Epoch 328/350\n",
      "93/93 [==============================] - 0s 66us/step - loss: 0.9600 - acc: 0.6237 - val_loss: 1.3019 - val_acc: 0.5417\n",
      "Epoch 329/350\n",
      "93/93 [==============================] - 0s 80us/step - loss: 0.9590 - acc: 0.6237 - val_loss: 1.3030 - val_acc: 0.5417\n",
      "Epoch 330/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 0.9577 - acc: 0.6344 - val_loss: 1.3030 - val_acc: 0.5417\n",
      "Epoch 331/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9570 - acc: 0.6344 - val_loss: 1.3031 - val_acc: 0.5417\n",
      "Epoch 332/350\n",
      "93/93 [==============================] - 0s 70us/step - loss: 0.9561 - acc: 0.6452 - val_loss: 1.3030 - val_acc: 0.5417\n",
      "Epoch 333/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 0.9548 - acc: 0.6344 - val_loss: 1.3032 - val_acc: 0.5417\n",
      "Epoch 334/350\n",
      "93/93 [==============================] - 0s 76us/step - loss: 0.9540 - acc: 0.6452 - val_loss: 1.3030 - val_acc: 0.5417\n",
      "Epoch 335/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 0.9530 - acc: 0.6452 - val_loss: 1.3035 - val_acc: 0.5417\n",
      "Epoch 336/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 0.9519 - acc: 0.6452 - val_loss: 1.3040 - val_acc: 0.5417\n",
      "Epoch 337/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9512 - acc: 0.6452 - val_loss: 1.3030 - val_acc: 0.5417\n",
      "Epoch 338/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 0.9495 - acc: 0.6452 - val_loss: 1.3031 - val_acc: 0.5417\n",
      "Epoch 339/350\n",
      "93/93 [==============================] - 0s 73us/step - loss: 0.9486 - acc: 0.6452 - val_loss: 1.3047 - val_acc: 0.5417\n",
      "Epoch 340/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 0.9476 - acc: 0.6452 - val_loss: 1.3052 - val_acc: 0.5417\n",
      "Epoch 341/350\n",
      "93/93 [==============================] - 0s 77us/step - loss: 0.9462 - acc: 0.6452 - val_loss: 1.3058 - val_acc: 0.5417\n",
      "Epoch 342/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 0.9446 - acc: 0.6452 - val_loss: 1.3055 - val_acc: 0.5417\n",
      "Epoch 343/350\n",
      "93/93 [==============================] - 0s 78us/step - loss: 0.9435 - acc: 0.6452 - val_loss: 1.3058 - val_acc: 0.5417\n",
      "Epoch 344/350\n",
      "93/93 [==============================] - 0s 82us/step - loss: 0.9425 - acc: 0.6452 - val_loss: 1.3056 - val_acc: 0.5417\n",
      "Epoch 345/350\n",
      "93/93 [==============================] - 0s 67us/step - loss: 0.9412 - acc: 0.6559 - val_loss: 1.3043 - val_acc: 0.5417\n",
      "Epoch 346/350\n",
      "93/93 [==============================] - 0s 72us/step - loss: 0.9407 - acc: 0.6559 - val_loss: 1.3043 - val_acc: 0.5417\n",
      "Epoch 347/350\n",
      "93/93 [==============================] - 0s 68us/step - loss: 0.9396 - acc: 0.6559 - val_loss: 1.3050 - val_acc: 0.5417\n",
      "Epoch 348/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9381 - acc: 0.6452 - val_loss: 1.3047 - val_acc: 0.5417\n",
      "Epoch 349/350\n",
      "93/93 [==============================] - 0s 74us/step - loss: 0.9372 - acc: 0.6559 - val_loss: 1.3042 - val_acc: 0.5417\n",
      "Epoch 350/350\n",
      "93/93 [==============================] - 0s 71us/step - loss: 0.9373 - acc: 0.6452 - val_loss: 1.3036 - val_acc: 0.5417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea49ee1a90>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=11, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "\n",
    "model.add(Dense(5, activation='softmax')) \n",
    "                                            \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the NN to the Training set\n",
    "model.fit(prediction_input_preprocessor.transform(X_train), pd.get_dummies(y_train), \n",
    "               batch_size = 65, \n",
    "               epochs = 350, validation_split=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "I2yehN9qZVBX",
    "outputId": "94201f97-af6a-40e9-a1c1-7d008160b2bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 4 3 0 0 2 2 1 0 3 2 3 1 4 4 4 1 4 3 3 3 1 2 2 1 0 4 0 2 1 4 2 4 4\n",
      " 4 1]\n",
      "['High', 'High', 'Average', 'High', 'Very Low', 'Very High', 'Average', 'Average', 'Low', 'Low', 'High', 'Average', 'Very High', 'Low', 'Very High', 'High', 'Very Low', 'Very Low', 'Very Low', 'High', 'Very Low', 'Very High', 'Very High', 'Very High', 'High', 'Low', 'Low', 'High', 'Average', 'Very Low', 'Average', 'Low', 'High', 'Very Low', 'Low', 'Very Low', 'Very Low', 'Very Low', 'High']\n"
     ]
    }
   ],
   "source": [
    "# using predict_classes() for multi-class data to return predicted class index.\n",
    "\n",
    "print(model.predict_classes(prediction_input_preprocessor.transform(X_test)))\n",
    "\n",
    "## Estas son mis clases.\n",
    "prediction_index=model.predict_classes(prediction_input_preprocessor.transform(X_test))\n",
    "\n",
    "#Now lets run some code to get keras to return the label rather than the index...\n",
    "\n",
    "# get labels from one hot encoded y_train data\n",
    "labels=pd.get_dummies(y_train).columns\n",
    "\n",
    "# Function to use to return label from column index location\n",
    "def index_to_label(labels,index_n): \n",
    "    return labels[index_n]\n",
    "    \n",
    "# Example: return label at predicted index location 1\n",
    "index_to_label(labels,1)\n",
    "\n",
    "# Iterate through all predicted indices using map method\n",
    "\n",
    "predicted_labels=list(map(lambda x: labels[x], prediction_index))\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "Ow1Ju7xBYTJS",
    "outputId": "f4b38092-a910-47ee-eb9d-18c92898b957"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.46746</td>\n",
       "      <td>0.48381</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
       "0  0.461538   0.46746    0.48381  0.477778    0     0    0   0"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating Keras Model\n",
    "\n",
    "def model_eval_metrics(y_true, y_pred,classification=\"TRUE\"):\n",
    "     if classification==\"TRUE\":\n",
    "        accuracy_eval = accuracy_score(y_true, y_pred)\n",
    "        f1_score_eval = f1_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
    "        precision_eval = precision_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
    "        recall_eval = recall_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
    "        mse_eval = 0\n",
    "        rmse_eval = 0\n",
    "        mae_eval = 0\n",
    "        r2_eval = 0\n",
    "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
    "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
    "     else:\n",
    "        accuracy_eval = 0\n",
    "        f1_score_eval = 0\n",
    "        precision_eval = 0\n",
    "        recall_eval = 0\n",
    "        mse_eval = mean_squared_error(y_true, y_pred)\n",
    "        rmse_eval = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae_eval = mean_absolute_error(y_true, y_pred)\n",
    "        r2_eval = r2_score(y_true, y_pred)\n",
    "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
    "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
    "     return finalmetricdata\n",
    "\n",
    "model_eval_metrics( y_test,predicted_labels,classification=\"TRUE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEXmoxfbCIYk"
   },
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HEx-NLzlyMcK",
    "outputId": "23be75d4-e1df-44c2-a4c0-6c6de65d3ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier's cross validation accuracy: 0.565909090909091\n",
      "Random Forest Classifier's Test-Data prediction accuracy: 0.41026\n"
     ]
    }
   ],
   "source": [
    "rf_m=RandomForestClassifier(n_estimators=1000, random_state = 4)\n",
    "\n",
    "rf_m.fit(prediction_input_preprocessor.transform(X_train), y_train)\n",
    "y_pred=rf_m.predict(prediction_input_preprocessor.transform(X_test))\n",
    "\n",
    "print(\"Random Forest Classifier's cross validation accuracy:\", np.mean(cross_val_score(rf_m, prediction_input_preprocessor.transform(X_train), y_train, cv=10)))\n",
    "print(\"Random Forest Classifier's Test-Data prediction accuracy: {:.5f}\".format(rf_m.score(prediction_input_preprocessor.transform(X_test), y_test)))\n",
    "\n",
    "pickle.dump(rf_m, open( \"rf_model.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "odADm-rJxc2t",
    "outputId": "b561056f-0e14-426e-96ef-6a2aa6ab9443"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.420463</td>\n",
       "      <td>0.499603</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
       "0  0.410256  0.420463   0.499603  0.416667    0     0    0   0"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelevalobject=model_eval_metrics(y_test,y_pred,classification=\"TRUE\")\n",
    "modelevalobject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mN6VMUDoCHZN"
   },
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EP5Xc8fgB8bg",
    "outputId": "7631fcad-589f-4695-932d-ed2161c28a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC's cross validation accuracy: 0.5143939393939393\n",
      "SVC's Test-Data prediction accuracy: 0.46154\n"
     ]
    }
   ],
   "source": [
    "svc_m = SVC(kernel='rbf', class_weight='balanced')\n",
    "svc_m.fit(prediction_input_preprocessor.transform(X_train), y_train)\n",
    "y_pred=svc_m.predict(prediction_input_preprocessor.transform(X_test))\n",
    "\n",
    "print(\"SVC's cross validation accuracy:\", np.mean(cross_val_score(svc_m, prediction_input_preprocessor.transform(X_train), y_train, cv=10)))\n",
    "print(\"SVC's Test-Data prediction accuracy: {:.5f}\".format(svc_m.score(prediction_input_preprocessor.transform(X_test), y_test)))\n",
    "\n",
    "pickle.dump(svc_m, open( \"svc_model.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "_ZhuVWKKxfj8",
    "outputId": "183cd9ac-f002-48cc-b0f6-ce24425d02c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.466215</td>\n",
       "      <td>0.538492</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision  recall  mse  rmse  mae  r2\n",
       "0  0.461538  0.466215   0.538492   0.475    0     0    0   0"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelevalobject=model_eval_metrics(y_test,y_pred,classification=\"TRUE\")\n",
    "modelevalobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "ABYAd2xAp1lH",
    "outputId": "074a0700-e793-405f-ba7f-3f89cb097daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso's cross validation accuracy: 0.5742424242424242\n",
      "Lasso's Test-Data prediction accuracy: 0.46154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "log_m= LogisticRegression(penalty=\"l1\", solver=\"saga\")\n",
    "log_m.fit(prediction_input_preprocessor.transform(X_train), y_train)\n",
    "y_pred=log_m.predict(prediction_input_preprocessor.transform(X_test))\n",
    "\n",
    "print(\"Lasso's cross validation accuracy:\", np.mean(cross_val_score(log_m, prediction_input_preprocessor.transform(X_train), y_train, cv=10)))\n",
    "print(\"Lasso's Test-Data prediction accuracy: {:.5f}\".format(log_m.score(prediction_input_preprocessor.transform(X_test), y_test)))\n",
    "\n",
    "pickle.dump(log_m, open( \"log_model.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "TZfR4nmpxnuE",
    "outputId": "f1f23dc9-e191-4837-b8ce-b6575bbe65aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.464161</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
       "0  0.461538  0.464161   0.555556  0.483333    0     0    0   0"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelevalobject=model_eval_metrics(y_test,y_pred,classification=\"TRUE\")\n",
    "modelevalobject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wmb0u_Onxpzg"
   },
   "source": [
    "#### 3.2 Submit your best model to the leader board for the World Happiness AI Model Share competition.\n",
    "\n",
    "##### You have the option to discuss these models in your report, but it is not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "WMNmBRECy1EI",
    "outputId": "30727845-6557-4b4f-cceb-40ce952e903d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras2onnx in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.17.5)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.6.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.21.0)\n",
      "Requirement already satisfied: onnxconverter-common>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.6.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.6.6)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (45.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install keras2onnx\n",
    "! pip3 install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9nG0yyAy7us"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('mymodel.onnx'):\n",
    "    from keras2onnx import convert_keras\n",
    "    onx = convert_keras(model, 'mymodel.onnx')\n",
    "    with open(\"mymodel.onnx\", \"wb\") as f:\n",
    "        f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86JkABcQzrbM"
   },
   "outputs": [],
   "source": [
    "aws_key_password_region = pickle.load( open( \"worldhappiness_modelsubmission_keys.pkl\", \"rb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uY2rBSraw0-B"
   },
   "outputs": [],
   "source": [
    "# Example Model Pre-launched into Model Share Site\n",
    "apiurl=\"https://btuvanmi55.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "region='us-east-1'\n",
    "model_filepath=\"mymodel.onnx\"   \n",
    "preprocessor_filepath=\"preprocessor.pkl\"\n",
    "preprocessor=\"TRUE\"\n",
    "\n",
    "trainingdata=X_train\n",
    "\n",
    "# Set aws keys for this project (these keys give you access to collaborate on a single project)\n",
    "\n",
    "#Importing from object that stores keys so we do not print out keys for others to see.\n",
    "\n",
    "aws_key_password_region = pickle.load( open( \"worldhappiness_modelsubmission_keys.pkl\", \"rb\" ) )\n",
    "\n",
    "aws_key=aws_key_password_region[0]\n",
    "aws_password=aws_key_password_region[1]\n",
    "region=aws_key_password_region[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dwLIiz2Oz2xt",
    "outputId": "4e7a0390-3186-42ce-bf01-0718fb45a015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please review model details and then resubmit to update this model.\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.submit_model(model_filepath=model_filepath, model_eval_metrics=modelevalobject,apiurl=apiurl, username=username, password=password, aws_key=aws_key,aws_password=aws_password, region=region, trainingdata=trainingdata,preprocessor_filepath=preprocessor_filepath,preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqjBHFNhruWf"
   },
   "source": [
    "Submit SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhrplRt30B7Q"
   },
   "source": [
    "### Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "zcR39xWo0Ay0",
    "outputId": "05f79396-6a84-4e31-cfe3-8528749a13fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEADERBOARD RANKINGS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "      <th>username</th>\n",
       "      <th>model_version</th>\n",
       "      <th>avg_ranking_classification</th>\n",
       "      <th>avg_ranking_regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3scman</td>\n",
       "      <td>85</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.713796</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3scman</td>\n",
       "      <td>70</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.675975</td>\n",
       "      <td>0.754286</td>\n",
       "      <td>0.700952</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dhoward97</td>\n",
       "      <td>69</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.700397</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3scman</td>\n",
       "      <td>62</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.642381</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.682273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SUN-Wenjun</td>\n",
       "      <td>83</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337698</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>username2</td>\n",
       "      <td>3</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337698</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>username2</td>\n",
       "      <td>2</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337698</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>username2</td>\n",
       "      <td>6</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.336015</td>\n",
       "      <td>0.354048</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>raquel904</td>\n",
       "      <td>102</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.336015</td>\n",
       "      <td>0.354048</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>raquel904</td>\n",
       "      <td>101</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  ...  avg_ranking_classification  avg_ranking_regression\n",
       "26  0.717949  0.717857  ...                    2.333333                     1.0\n",
       "16  0.717949  0.713796  ...                    2.333333                     1.0\n",
       "8   0.666667  0.675975  ...                    2.666667                     1.0\n",
       "62  0.692308  0.693333  ...                    4.000000                     1.0\n",
       "66  0.641026  0.642381  ...                    4.000000                     1.0\n",
       "..       ...       ...  ...                         ...                     ...\n",
       "85  0.333333  0.337698  ...                   40.000000                     1.0\n",
       "75  0.333333  0.337698  ...                   40.000000                     1.0\n",
       "47  0.333333  0.337698  ...                   40.000000                     1.0\n",
       "54  0.333333  0.336015  ...                   40.666667                     1.0\n",
       "31  0.333333  0.336015  ...                   40.666667                     1.0\n",
       "\n",
       "[106 rows x 12 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "apiurl=\"https://btuvanmi55.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "leaderboard = ai.get_leaderboard(apiurl, username, password, aws_key, aws_password, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link REPO: https://github.com/cecicabello/firstassignment"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Advanced ML HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
